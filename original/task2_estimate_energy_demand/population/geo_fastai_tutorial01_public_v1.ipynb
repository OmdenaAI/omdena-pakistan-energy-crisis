{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geo-fastai-tutorial01-public-v1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addXbl69ryvc",
        "colab_type": "text"
      },
      "source": [
        "# How to Segment Buildings on Drone Imagery with Fast.ai & Cloud-Native GeoData Tools\n",
        "\n",
        "## An Interactive Intro to Geospatial Deep Learning on Google Colab \n",
        "\n",
        "**by [@daveluo](https://github.com/daveluo)**\n",
        "\n",
        "In this Google Colab notebook and accompanying [Medium post](https://medium.com/@anthropoco/how-to-segment-buildings-on-drone-imagery-with-fast-ai-cloud-native-geodata-tools-ae249612c321?source=friends_link&sk=57b82002ac47724ecf9a2aaa98de994b), we will learn all the code and concepts comprising a complete workflow to automatically detect and delineate building footprints (instance segmentation) from drone imagery with cutting edge deep learning models. \n",
        "\n",
        "All you'll need is a Google account, an internet connection, and a couple of hours to learn how to make the data & model that learns to make something like [this](https://alpha.anthropo.co/znz-demo):\n",
        "\n",
        "![zanzibar demo](https://www.dropbox.com/s/vv6ebxkd3x7xxfy/zanzibar_banner.png?dl=1)\n",
        "\n",
        "## In modular steps, we'll learn to…\n",
        "\n",
        "### Preprocess image geoTIFFs and manually labeled data geoJSON files into training data for deep learning:\n",
        "\n",
        "![geotiff and geojson](https://cdn-images-1.medium.com/max/1200/1*Myn-7f-tLhaMRaaNYg1oHw.png)\n",
        "\n",
        "### Create a U-net segmentation model to predict what pixels in an image represent buildings (and building-related features):\n",
        "\n",
        "![segmentation pred vs actual](https://cdn-images-1.medium.com/max/1200/1*4qsYToRH8Q-riSFtxuNWIg.png)\n",
        "\n",
        "### Test our model's performance on unseen imagery with GPU or CPU:\n",
        "\n",
        "![gpu inference](https://cdn-images-1.medium.com/max/900/1*ag6ERcdl-K1-Dj6ddyrBMA.png)\n",
        "![cpu inference](https://cdn-images-1.medium.com/max/900/1*kNulsQQdDJAD5xN9Q8qa_A.png)\n",
        "\n",
        "### Post-process raw model outputs into geo-registered building shapes evaluated against ground truth:\n",
        "\n",
        "![eval](https://cdn-images-1.medium.com/max/1200/0*D47afqJ_7l54o-s3)\n",
        "\n",
        "### And along the way, we'll get familiar with great geospatial data & deep learning tools/resources like:\n",
        "\n",
        "- [Geopandas](http://geopandas.org/): \"an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types.\"\n",
        "- [Rasterio](https://github.com/mapbox/rasterio): \"reads and writes geospatial raster datasets\"\n",
        "- [Supermercado](https://github.com/mapbox/supermercado): \"supercharger for Mercantile\" (spherical mercator tile and coordinate utilities)\n",
        "- [Rio-tiler](https://github.com/cogeotiff/rio-tiler): \"Rasterio plugin to read mercator tiles from Cloud Optimized GeoTIFF dataset\"\n",
        "- [Solaris](https://github.com/CosmiQ/solaris): \"Geospatial Machine Learning Analysis Toolkit\" by Cosmiq Works\n",
        "- [Cloud-Optimized GeoTIFFs (COG)](https://www.cogeo.org/): \"An imagery format for cloud-native geospatial processing\"\n",
        "- [Spatio-Temporal Asset Catalogs (STAC)](https://stacspec.org/): \"Enabling online search and discovery of geospatial assets\"\n",
        "- [OpenAerialMap](https://openaerialmap.org/): \"The open collection of aerial imagery\"\n",
        "- [Fast.ai](http://fast.ai/) for [geospatial deep learning](https://forums.fast.ai/t/geospatial-deep-learning-resources-study-group/31044): \"The fastai library simplifies training fast and accurate neural nets using modern best practices\" built on the [PyTorch](https://pytorch.org/) deep learning platform.\n",
        "\n",
        "### How to get the most out of this tutorial:\n",
        "\n",
        "This Colab notebook is our main learning resource - working interactively here is highly recommended!\n",
        "\n",
        "Code is organized into modular sections, set up for installation/import of all required dependencies, and executable on either CPU or GPU runtimes (depending on the section). Links to load files generated at each step are also included so you can pick up and start from any section. Inline# comments (& references for further reading) are provided within code cells to explain steps or nuances in more detail as needed. Executing all code cells end-to-end takes <1 hour on GPU.\n",
        "\n",
        "The Medium post serves as a high-level conceptual walkthrough and maps directly to sections within the Colab notebook. The post works best as a quick overview with handy bookmarks to Colab or viewed side-by-side with this Colab notebook as a code & concept companion set.\n",
        "\n",
        "This tutorial assumes you have a working knowledge of Python, data analysis with Pandas, making training/validation/test sets for machine learning, and a beginner practitioner's grasp of deep learning concepts. Or the motivation to gain what knowledge you're missing by following the ample references linked throughout this post and notebook.\n",
        "\n",
        "### With that as mental prep, let's do some geospatial deep learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FuBWTy4sUI8",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Processing\n",
        "\n",
        "**Note that the preprocessing section is possible to be done on CPU runtime:**\n",
        "\n",
        "Change in menu: Runtime > Change runtime type > Hardware Accelerator = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynKRK2o6YyoM",
        "colab_type": "text"
      },
      "source": [
        "## Install all the geo things\n",
        "\n",
        "`Pip install` the required geodata processing packages we'll be using of, test that their import to Colab works, and create our output data directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKapBGDLQwSq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d4dce524-e793-4ac2-9e9d-92a4c46b82c8"
      },
      "source": [
        "!add-apt-repository ppa:ubuntugis/ppa -y\n",
        "!apt-get update\n",
        "!apt-get install python-numpy gdal-bin libgdal-dev\n",
        "!apt install python3-rtree\n",
        "\n",
        "!pip install rasterio\n",
        "!pip install geopandas\n",
        "!pip install descartes\n",
        "!pip install solaris\n",
        "!pip install rio-tiler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Official stable UbuntuGIS packages.\n",
            "\n",
            "\n",
            " More info: https://launchpad.net/~ubuntugis/+archive/ubuntu/ppa\n",
            "Press [ENTER] to continue or Ctrl-c to cancel adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctpzFFb6MIdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for bleeding edge version of solaris:\n",
        "# !pip install git+https://github.com/CosmiQ/solaris/@dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K54DsLyK2O71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import solaris as sol\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "import rasterio\n",
        "import os\n",
        "\n",
        "data_dir = Path('data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "img_path = data_dir/'images-256'\n",
        "mask_path = data_dir/'masks-256'\n",
        "img_path.mkdir(exist_ok=True)\n",
        "mask_path.mkdir(exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRtz_4ue_R4g",
        "colab_type": "text"
      },
      "source": [
        "## Preview and load imagery and labels\n",
        "\n",
        "For this tutorial, we'll use the [Tanzania Open AI Challenge dataset](https://competitions.codalab.org/competitions/20100#learn_the_details) of 7-cm resolution drone imagery and building footprint labels over Unguja Island, Zanzibar. \n",
        "\n",
        "Much thanks to the following organizations for producing, openly licensing, and making this invaluable dataset accessible:\n",
        "\n",
        "- [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) licensed by Commission for Lands (COLA) - Revolutionary Government of Zanzibar (RGoZ)\n",
        "- Labeled data produced & processed by State University of Zanzibar (SUZA), [World Bank OpenDRI](https://opendri.org/project/zanzibar/), [WeRobotics](https://werobotics.org/)\n",
        "- Drone imagery created by [Zanzibar Mapping Initiative](http://www.zanzibarmapping.com/)  and hosted on [OpenAerialMap](https://map.openaerialmap.org/#/39.40040588378906,-5.980094945523311,10/square/3001121111?_k=34xcng):\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1200/1*-kSPTKsU5vF2c9NEKjOqNQ.png)\n",
        "\n",
        "For simplicity of demonstration, we'll create training and validation data from a single drone image (in cloud-optimized geoTIFF format) and its accompanying ground-truth labels of manually traced building outlines (in GeoJSON format).\n",
        "\n",
        "We'll work with imagery and labels from image grid `znz001` which covers the northern tip of Zanzibar's main island of Unguja. Here is a [browsable preview](https://geoml-samples.netlify.com/item/9Eiufow7wPXLqQEP1Di2J5X8kXkBLgMsCBoN37VrtRPB/2sEaEKnnyjG2mx7CnN1ESAdjYAEQjoNRxSxTjc4vPGR?si=0&t=preview#15/-5.732621/39.301114) of the drone imagery with its building footprint labels, organized per the Spatio-Temporal Asset Catalog ([STAC](https://github.com/radiantearth/stac-spec/)) [label extension](https://github.com/radiantearth/stac-spec/tree/dev/extensions/label) and visualized in an instance of [STAC browser](https://github.com/radiantearth/stac-browser):\n",
        "\n",
        "![znz001-preview](https://www.dropbox.com/s/lfrzpgukk922jtr/stacpreview-znz001.png?dl=1)\n",
        "\n",
        "After previewing the labeled data and imagery, let's import our geo-processing tools, let's copy the direct download URLs from the Assets tab of the browser and test loading them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AT4z0n5_sYeb",
        "colab": {}
      },
      "source": [
        "tif_url = 'http://oin-hotosm.s3.amazonaws.com/5afeda152b6a08001185f11a/0/5afeda152b6a08001185f11b.tif'\n",
        "geojson_url = 'https://www.dropbox.com/sh/ct3s1x2a846x3yl/AAARCAOqhcRdoU7ULOb9GJl9a/grid_001.geojson?dl=1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "avIXcLzdsYem",
        "colab": {}
      },
      "source": [
        "rasterio.open(tif_url).meta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BQLIcBJSGbyV",
        "colab": {}
      },
      "source": [
        "# TODO: bug with rasterio/gdal not loading https urls, workaround by using http: urls or download file locally\n",
        "# !export CURL_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\n",
        "!wget -O tmp.tif {tif_url}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0JbwB5JnhJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rasterio.open('tmp.tif').meta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RC5HvDwEiuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load geojson for znz001 labels\n",
        "\n",
        "label_df = gpd.read_file(geojson_url)\n",
        "label_df = label_df[label_df['geometry'].isna() != True] # remove empty rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkmRrN3GFsRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_df.plot(figsize=(10,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JHpicpLAZOn",
        "colab_type": "text"
      },
      "source": [
        "## Draw train and validation areas of interest (AOIs) with geojson.io\n",
        "\n",
        "Since we are working with a single image, we need to delineate what sub-areas of the image and labels should be used as training versus validation data for model training.\n",
        "\n",
        "Using [geojson.io](http://geojson.io), we'll draw our `trn` and `val` Areas of Interest (AOI) polygons in geojson format and add `dataset:trn` or `dataset:val` to the respective polygon `properties`.\n",
        "\n",
        "The finished polygons look something like this in geojson.io:\n",
        "![alt text](https://www.dropbox.com/s/v8u8ihnuj6b5lbl/geojson_screenshot3.png?dl=1)\n",
        "\n",
        "And here is the exact GeoJSON file I created viewable in geojson.io:\n",
        "http://geojson.io/#id=gist:daveluo/8e192744b2aa377db162bc34e0e0ae64&map=15/-5.7314/39.3026\n",
        "\n",
        "**protip:** in geojson.io, you can display the drone imagery as a base layer via the menu: Meta > Add map layer > Layer URL: https://tiles.openaerialmap.org/5b100d4b2b6a08001185f344/0/5b100d4b2b6a08001185f345/{z}/{x}/{y}.png\n",
        "\n",
        "In this case, I intentionally drew a more complex shape for each AOI to demonstrate some later steps but we could have drawn simpler adjacent rectanFor demonstration of later steps, I intentionally drew a more complex shape for each AOI but we could have simply drawn adjacent rectangles instead.\n",
        "Or in more complex cases, we could choose to draw AOIs of smaller sub-areas that don't encompass the entire image - for instance, if we want to create training data for specific types of environments like dense urban areas or sparsely populated rural areas only or we want to avoid using poorly labeled areas in our training data.\n",
        "\n",
        "Drawing the AOIs as geoJSON polygons in this way gives us the flexibility to choose exactly what and where our training and validation data represents.gles instead. \n",
        "\n",
        "Or we could choose to  draw AOIs of smaller areas that don't encompass the entire image if we want to only create training/validation data for specific types of environments (like dense urban areas only)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQJZVc7YCkkL",
        "colab_type": "text"
      },
      "source": [
        "## Convert train and validation AOIs to slippy map tile polygons with supermercado and geopandas\n",
        "\n",
        "In this step, we'll use [supermercado](https://github.com/mapbox/supermercado) to generate square polygons representing all the [slippy map tiles](https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames) at a specified zoom level that overlap the geojson training and validation AOIs we created above. \n",
        "\n",
        "For this tutorial, we'll work with slippy map tiles of `tile_size=256` and `zoom_level=19` which yields a manageable number of tiles and satisfactory segmentation results without too much preprocessing or model training time.  \n",
        "\n",
        "You could also try setting a higher or lower `zoom_level` which would generate more or less tiles at higher or lower resolutions respectively. \n",
        "\n",
        "Here is an example of different tile `zoom_levels` over the same area of Zanzibar (see the round, white satellite TV dish for a consistently sized visual reference):\n",
        "\n",
        "![zoom level comparison](https://cdn-images-1.medium.com/max/1200/1*06aV0V5_-uu0_mQCe13sBA.png)\n",
        "\n",
        "Learn more about slippy maps [here](https://wiki.openstreetmap.org/wiki/Slippy_Map), [here](https://developers.planet.com/tutorials/slippy-maps-101/), and [here](https://wiki.openstreetmap.org/wiki/Zoom_levels). \n",
        "\n",
        "\n",
        "Then we'll merge our supermercado-generated slippy map tile polygons into one `GeoDataFrame` with [geopandas](http://geopandas.org/). We'll also check for and reconcile overlapping train and validation tiles which would otherwise throw off how we evaluate our progress with model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc8A4S-q4QOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download pre-made AOI geojson file:\n",
        "!wget -O aoi.geojson https://www.dropbox.com/s/ojyjvvoer5guadr/znz001_trnval2.geojson?dl=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzgNnmsZ_BWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tile_size = 256\n",
        "zoom_level = 19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3s34_Os7Pyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aoi_df = gpd.read_file('aoi.geojson')\n",
        "aoi_df.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g2eoGj77dEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aoi_df[aoi_df['dataset']=='trn']['geometry'].to_file('trn_aoi.geojson', driver='GeoJSON')\n",
        "aoi_df[aoi_df['dataset']=='val']['geometry'].to_file('val_aoi.geojson', driver='GeoJSON')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coiMcKgi67B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see https://github.com/mapbox/supermercado#supermercado-burn\n",
        "!cat trn_aoi.geojson | supermercado burn {zoom_level} | mercantile shapes | fio collect > trn_aoi_z{zoom_level}tiles.geojson\n",
        "!cat val_aoi.geojson | supermercado burn {zoom_level} | mercantile shapes | fio collect > val_aoi_z{zoom_level}tiles.geojson"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZ6qSGJ7H6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_tiles = gpd.read_file(f'trn_aoi_z{zoom_level}tiles.geojson')\n",
        "val_tiles = gpd.read_file(f'val_aoi_z{zoom_level}tiles.geojson')\n",
        "trn_tiles['dataset'] = 'trn'\n",
        "val_tiles['dataset'] = 'val'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftfFsMCD7KzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see if there's overlapping tiles between trn and val\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "trn_tiles.plot(ax=ax, color='grey', alpha=0.5, edgecolor='red')\n",
        "val_tiles.plot(ax=ax, color='grey', alpha=0.5, edgecolor='blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Red4FeYiEUiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge into one gdf to keep all trn tiles while dropping overlapping/duplicate val tiles\n",
        "import pandas as pd\n",
        "tiles_gdf = gpd.GeoDataFrame(pd.concat([trn_tiles, val_tiles], ignore_index=True), crs=trn_tiles.crs)\n",
        "tiles_gdf.drop_duplicates(subset=['id'], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMzwpQc4FwyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check that there's no more overlapping tiles between trn and val\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "tiles_gdf[tiles_gdf['dataset'] == 'trn'].plot(ax=ax, color='grey', edgecolor='red', alpha=0.5)\n",
        "tiles_gdf[tiles_gdf['dataset'] == 'val'].plot(ax=ax, color='grey', edgecolor='blue', alpha=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crx0d3Es7L4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tiles_gdf.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_eHsusz7Xxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert 'id' string to list of ints for z,x,y\n",
        "\n",
        "def reformat_xyz(tile_gdf):\n",
        "  tile_gdf['xyz'] = tile_gdf.id.apply(lambda x: x.lstrip('(,)').rstrip('(,)').split(','))\n",
        "  tile_gdf['xyz'] = [[int(q) for q in p] for p in tile_gdf['xyz']]\n",
        "  return tile_gdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA2pxJ9wDaYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tiles_gdf = reformat_xyz(tiles_gdf)\n",
        "tiles_gdf.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MuNmPfAEFPD",
        "colab_type": "text"
      },
      "source": [
        "## Load slippy map tile image from COG with rio-tiler and corresponding label with geopandas\n",
        "\n",
        "Now we'll use  [rio-tiler](https://github.com/cogeotiff/rio-tiler) and the slippy map tile polygons generated by supermercado to test load a single 256x256 pixel tile from our znz001 COG image file. We will also load the znz001 geoJSON labels into a geopandas GeoDataFrame and crop  the building geometries to only those that intersect the bounds of the tile image.\n",
        "\n",
        "Here is a great intro to COGs, rio-tiler, and exciting developments in the cloud-native geospatial toolbox by [Vincent Sarago](https://medium.com/@_VincentS_) of [Development Seed](https://developmentseed.org/): https://medium.com/devseed/cog-talk-part-1-whats-new-941facbcd3d1\n",
        "\n",
        "We'll then create our corresponding 3-channel RGB mask by passing these cropped geometries to solaris' df_to_px_mask function. Pixel value of 255 in the generated mask: \n",
        "\n",
        "- in the 1st (Red) channel represent building footprints, \n",
        "- in the 2nd (Green) channel represent building boundaries (visually looks yellow on the RGB mask display because the pixels overlap red and green+red=yellow), \n",
        "- and in the 3rd (Blue) channel represent close contact points between adjacent buildings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EUBUOI0DdYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rio_tiler import main as rt_main\n",
        "\n",
        "# import mercantile\n",
        "from rasterio.transform import from_bounds\n",
        "from shapely.geometry import Polygon\n",
        "from shapely.ops import cascaded_union"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5COhcXRJEW1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 220\n",
        "tiles_gdf.iloc[idx]['xyz']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFOLQPvrESv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tile, mask = rt_main.tile(tif_url, *tiles_gdf.iloc[idx]['xyz'], tilesize=tile_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r77SR7rFEdRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(np.moveaxis(tile,0,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-JuoSH4G0wc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# redisplay our labeled geojson file\n",
        "label_df.plot(figsize=(10,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q22YqM-LH44W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the geometries from the geodataframe\n",
        "all_polys = label_df.geometry"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aher4OSDK4eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preemptively fix and merge any invalid or overlapping geoms that would otherwise throw errors during the rasterize step. \n",
        "# TODO: probably a better way to do this\n",
        "\n",
        "# https://gis.stackexchange.com/questions/271733/geopandas-dissolve-overlapping-polygons\n",
        "# https://nbviewer.jupyter.org/gist/rutgerhofste/6e7c6569616c2550568b9ce9cb4716a3\n",
        "\n",
        "def explode(gdf):\n",
        "    \"\"\"    \n",
        "    Will explode the geodataframe's muti-part geometries into single \n",
        "    geometries. Each row containing a multi-part geometry will be split into\n",
        "    multiple rows with single geometries, thereby increasing the vertical size\n",
        "    of the geodataframe. The index of the input geodataframe is no longer\n",
        "    unique and is replaced with a multi-index. \n",
        "\n",
        "    The output geodataframe has an index based on two columns (multi-index) \n",
        "    i.e. 'level_0' (index of input geodataframe) and 'level_1' which is a new\n",
        "    zero-based index for each single part geometry per multi-part geometry\n",
        "    \n",
        "    Args:\n",
        "        gdf (gpd.GeoDataFrame) : input geodataframe with multi-geometries\n",
        "        \n",
        "    Returns:\n",
        "        gdf (gpd.GeoDataFrame) : exploded geodataframe with each single \n",
        "                                 geometry as a separate entry in the \n",
        "                                 geodataframe. The GeoDataFrame has a multi-\n",
        "                                 index set to columns level_0 and level_1\n",
        "        \n",
        "    \"\"\"\n",
        "    gs = gdf.explode()\n",
        "    gdf2 = gs.reset_index().rename(columns={0: 'geometry'})\n",
        "    gdf_out = gdf2.merge(gdf.drop('geometry', axis=1), left_on='level_0', right_index=True)\n",
        "    gdf_out = gdf_out.set_index(['level_0', 'level_1']).set_geometry('geometry')\n",
        "    gdf_out.crs = gdf.crs\n",
        "    return gdf_out\n",
        "\n",
        "def cleanup_invalid_geoms(all_polys):\n",
        "  all_polys_merged = gpd.GeoDataFrame()\n",
        "  all_polys_merged['geometry'] = gpd.GeoSeries(cascaded_union([p.buffer(0) for p in all_polys]))\n",
        "\n",
        "  gdf_out = explode(all_polys_merged)\n",
        "  gdf_out = gdf_out.reset_index()\n",
        "  gdf_out.drop(columns=['level_0','level_1'], inplace=True)\n",
        "  all_polys = gdf_out['geometry']\n",
        "  return all_polys\n",
        "\n",
        "all_polys = cleanup_invalid_geoms(all_polys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjoU0JhcF1I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the same tile polygon as our tile image above\n",
        "tile_poly = tiles_gdf.iloc[idx]['geometry']\n",
        "print(tile_poly.bounds)\n",
        "tile_poly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "disyrRD_GEZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get affine transformation matrix for this tile using rasterio.transform.from_bounds: https://rasterio.readthedocs.io/en/stable/api/rasterio.transform.html#rasterio.transform.from_bounds\n",
        "tfm = from_bounds(*tile_poly.bounds, tile_size, tile_size) \n",
        "tfm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RktjSglgG18W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# crop znz001 geometries to what overlaps our tile polygon bounds\n",
        "cropped_polys = [poly for poly in all_polys if poly.intersects(tile_poly)]\n",
        "cropped_polys_gdf = gpd.GeoDataFrame(geometry=cropped_polys, crs={'init': 'epsg:4326'})\n",
        "cropped_polys_gdf.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgip8xNQUAnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# burn a footprint/boundary/contact 3-channel mask with solaris: https://solaris.readthedocs.io/en/latest/tutorials/notebooks/api_masks_tutorial.html\n",
        "\n",
        "fbc_mask = sol.vector.mask.df_to_px_mask(df=cropped_polys_gdf,\n",
        "                                         channels=['footprint', 'boundary', 'contact'],\n",
        "                                         affine_obj=tfm, shape=(tile_size,tile_size),\n",
        "                                         boundary_width=5, boundary_type='inner', contact_spacing=5, meters=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70yEgb6UUZNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10, 5))\n",
        "ax1.imshow(np.moveaxis(tile,0,2))\n",
        "ax2.imshow(fbc_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O7gPXO7UZY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(10, 5))\n",
        "ax1.imshow(fbc_mask[:,:,0])\n",
        "ax2.imshow(fbc_mask[:,:,1])\n",
        "ax3.imshow(fbc_mask[:,:,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36_uswSoVYbL",
        "colab_type": "text"
      },
      "source": [
        "## Make and save all the image and mask tiles\n",
        "\n",
        "Now that we've successfully loaded one tile image from COG with rio-tiler and created its 3-channel RGB mask with solaris, let's generate our full training and validation datasets. \n",
        "\n",
        "We'll write some functions and loops to run through all of our `trn` and `val` tiles at `zoom_level=19` and save them as lossless `png` files in the appropriate folders with a filename schema of `{save_path}/{prefix}{z}_{x}_{y}` so we can easily identify and geolocate what tile each file represents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpv2LjotY8l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skimage\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM89e09RWxOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_tile_img(tif_url, xyz, tile_size, save_path='', prefix='', display=False):\n",
        "  x,y,z = xyz\n",
        "  tile, mask = rt_main.tile(tif_url, x,y,z, tilesize=tile_size)\n",
        "  if display: \n",
        "    plt.imshow(np.moveaxis(tile,0,2))\n",
        "    plt.show()\n",
        "    \n",
        "  skimage.io.imsave(f'{save_path}/{prefix}{z}_{x}_{y}.png',np.moveaxis(tile,0,2), check_contrast=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t7aCsKcd5o1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_tile_mask(labels_poly, tile_poly, xyz, tile_size, save_path='', prefix='', display=False):\n",
        "  x,y,z = xyz\n",
        "  tfm = from_bounds(*tile_poly.bounds, tile_size, tile_size) \n",
        "  \n",
        "  cropped_polys = [poly for poly in labels_poly if poly.intersects(tile_poly)]\n",
        "  cropped_polys_gdf = gpd.GeoDataFrame(geometry=cropped_polys, crs={'init': 'epsg:4326'})\n",
        "  \n",
        "  fbc_mask = sol.vector.mask.df_to_px_mask(df=cropped_polys_gdf,\n",
        "                                         channels=['footprint', 'boundary', 'contact'],\n",
        "                                         affine_obj=tfm, shape=(tile_size,tile_size),\n",
        "                                         boundary_width=5, boundary_type='inner', contact_spacing=5, meters=True)\n",
        "  \n",
        "  if display: plt.imshow(fbc_mask); plt.show()\n",
        "  \n",
        "  skimage.io.imsave(f'{save_path}/{prefix}{z}_{x}_{y}_mask.png',fbc_mask, check_contrast=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs_Da05cabX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tiles_gdf[tiles_gdf['dataset'] == 'trn'].shape, tiles_gdf[tiles_gdf['dataset'] == 'val'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iXCxkbGY6vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we'll load our COG locally but could also load directly from url which is slower and subject to potentially more i/o issues\n",
        "# TODO: try loading from url and catch i/o exceptions\n",
        "# TODO: multithread/multiprocess this? Took ~3.5 mins to load and save 1261 image tiles on local COG file loading\n",
        "for idx, tile in tqdm(tiles_gdf.iterrows()):\n",
        "  dataset = tile['dataset']\n",
        "  save_tile_img('tmp.tif', tile['xyz'], tile_size, save_path=img_path, prefix=f'znz001{dataset}_', display=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8jEsBknewiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: multiprocess this? Took ~3 mins to burn and save 1261 masks\n",
        "for idx, tile in tqdm(tiles_gdf.iterrows()):\n",
        "  dataset = tile['dataset']\n",
        "  tile_poly = tile['geometry']\n",
        "  save_tile_mask(all_polys, tile_poly, tile['xyz'], tile_size, save_path=mask_path,prefix=f'znz001{dataset}_', display=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE6kzJlkq1-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check that tile images and masks saved correctly\n",
        "start_idx, end_idx = 200,205\n",
        "for i,j in zip(sorted(img_path.iterdir())[start_idx:end_idx], sorted(mask_path.iterdir())[start_idx:end_idx]):\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
        "  ax1.imshow(skimage.io.imread(i))\n",
        "  ax2.imshow(skimage.io.imread(j))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8EBCMDQghXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compress and download\n",
        "!tar -czf znz001trn.tar.gz data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L6CysYj7BWR",
        "colab_type": "text"
      },
      "source": [
        "## Save files to GDrive (or download to computer)\n",
        "\n",
        "Colab does not persistently store any files created and saved in its runtimes for than 8-12 hours (or less depending on inactivity or overall demand on the system). We'll transfer or download the files we create somewhere else. We can:\n",
        "\n",
        "1. **Mount our own account's Google Drive storage and transfer files** there via a `!cp` command: see below cell\n",
        "2. **Download  files to local computer:** go to Files tab on left > find and right-click selected file > click Download > file will be prepared by Colab and automatically downloaded when ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4w-FT4O7yUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to mount and transfer to GDrive: uncomment and run this and the next cell, follow instructions to auhorize access to your GDrive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7GG2up2-ykX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy training data compressed tarball to root of your GDrive\n",
        "# !cp znz001trn.tar.gz /content/drive/My\\ Drive/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALaLSS1WrsLu",
        "colab_type": "text"
      },
      "source": [
        "# Train u-net segmentation model with fastai & pytorch\n",
        "\n",
        "As our deep learning framework and library of tools, we'll use the excellent [fastai](https://github.com/fastai/fastai) library built on top of [PyTorch](https://pytorch.org/). \n",
        "\n",
        "For more info:\n",
        "- about Fast.ai, the organization: https://www.fast.ai/about/\n",
        "- direct links to the free MOOC series: \n",
        "  - Part 1 (\"Practical Deep Learning for Coders\"): https://course.fast.ai/index.html\n",
        "  - Part 2 (\"Deep Learning from the Foundations\"): https://course.fast.ai/part2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHcelzHYwy2F",
        "colab_type": "text"
      },
      "source": [
        "## Download and install fastai\n",
        "\n",
        "Let's download, install, and set up fastai v1 (currently at 1.0.55). And if we're not already on it, let's reset Colab to a GPU runtime (this removes locally stored files since it switches to a new environment so you will have to re-download and untar the training dataset created in above steps):\n",
        "\n",
        "**SWITCH TO GPU RUNTIME: Menu > Runtime > Change runtime type > Hardware Accelerator = GPU**\n",
        "\n",
        "Colab's free GPUs range from a Tesla K80, T4, or T8 depending on their availability. See the `===Hardware===` section of `show_install()` for what GPU type and how much GPU memory  is available which will affect the batch size and training time.\n",
        "\n",
        "For all of these GPUs and mem sizes, a batch size of `bs=16` at `size=256` should train at <2 mins/epoch without encountering out-of-memory issues but if it does comes up, lower the bs to 8.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yeMpwe8kunM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl https://course.fast.ai/setup/colab | bash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVW71v4gr2tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.vision import *\n",
        "from fastai.callbacks import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivqRgFKKsC7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.utils.collect_env import *\n",
        "show_install(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxx-MIOwtbA",
        "colab_type": "text"
      },
      "source": [
        "## Set up data\n",
        "\n",
        "Now we'll set up our training dataset of tile images and masks created above to load correctly into fastai for training and validation. \n",
        "\n",
        "The code in this step tracks closely with that of fastai course's lesson3-camvid so please refer to that [lesson video](https://course.fast.ai/videos/?lesson=3) and [notebook](https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson3-camvid.ipynb) for more detailed and excellent explanation by Jeremy Howard about the code and fastai's [Data Block API](https://docs.fast.ai/data_block.html).\n",
        "\n",
        "The main departures from the camvid lesson notebook is the use of filename string parsing to determine which image and mask files comprise the validation data.\n",
        "\n",
        "And we'll subclass `SegmentationLabelList` to alter the behavior of `open_mask` and `PIL.Image` underlying it in order to open the 3-channel target masks as RGB images `(convert_mode='RGB')` instead of default greyscale 1-channel images `(convert_mode='L')`.\n",
        "\n",
        "We'll also visually confirm that the image files and channels of the respective target mask file are loaded and paired correctly with a display function `show_3ch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTMm7rjqsXjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if not already present in file storage, download and extract the training/validation dataset created in above sections\n",
        "!wget -O znz001trn.tar.gz https://www.dropbox.com/s/2a2ikf7m265davv/znz001trn.tar.gz?dl=1\n",
        "!tar -xf znz001trn.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGH9PDVZsEJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = Path('data')\n",
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mybWls0qsLKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_lbl = path/'masks-256'\n",
        "path_img = path/'images-256'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7XTP3kjtQN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fnames = get_image_files(path_img)\n",
        "lbl_names = get_image_files(path_lbl)\n",
        "print(len(fnames), len(lbl_names))\n",
        "fnames[:3], lbl_names[:3] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VpoLon2tUZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_y_fn = lambda x: path_lbl/f'{x.stem}_mask.png'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5SrgkkntWH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test that masks are opening correctly with open_mask() settings\n",
        "img_f = fnames[121]\n",
        "img = open_image(img_f)\n",
        "mask = open_mask(get_y_fn(img_f), convert_mode='RGB', div=False)\n",
        "\n",
        "fig,ax = plt.subplots(1,1, figsize=(10,10))\n",
        "img.show(ax=ax)\n",
        "mask.show(ax=ax, alpha=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBgGrBDItkRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(mask.data.view(-1), bins=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Avc4q6uPx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the valdation set by fn prefix\n",
        "holdout_grids = ['znz001val_']\n",
        "valid_idx = [i for i,o in enumerate(fnames) if any(c in str(o) for c in holdout_grids)]\n",
        "print(len(valid_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lROxOrYNuXRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# subclassing SegmentationLabelList to set open_mask(fn, div=True, convert_mode='RGB') for 3 channel target masks\n",
        "\n",
        "class SegLabelListCustom(SegmentationLabelList):\n",
        "    def open(self, fn): return open_mask(fn, div=True, convert_mode='RGB')\n",
        "    \n",
        "class SegItemListCustom(SegmentationItemList):\n",
        "    _label_cls = SegLabelListCustom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGvhpoobuS0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the classes corresponding to each channel\n",
        "codes = np.array(['Footprint','Boundary','Contact'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfmC7w-2uLM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = 256\n",
        "bs = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRps_L51ukCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define image transforms for data augmentation and create databunch. More about image tfms and data aug at https://docs.fast.ai/vision.transform.html \n",
        "tfms = get_transforms(flip_vert=True, max_warp=0.1, max_rotate=20, max_zoom=2, max_lighting=0.3)\n",
        "\n",
        "src = (SegItemListCustom.from_folder(path_img)\n",
        "        .split_by_idx(valid_idx)\n",
        "        .label_from_func(get_y_fn, classes=codes))\n",
        "\n",
        "data = (src.transform(tfms, size=size, tfm_y=True)\n",
        "        .databunch(bs=bs)\n",
        "        .normalize(imagenet_stats))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XHSnA9iumWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_3ch(imgitem, figsize=(10,5)):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=figsize)\n",
        "    ax1.imshow(np.asarray(imgitem.data[0,None])[0])\n",
        "    ax2.imshow(np.asarray(imgitem.data[1,None])[0])\n",
        "    ax3.imshow(np.asarray(imgitem.data[2,None])[0])\n",
        "    \n",
        "    ax1.set_title('Footprint')\n",
        "    ax2.set_title('Boundary')\n",
        "    ax3.set_title('Contact')\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nygd_dpVuoqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(10,15):\n",
        "    print(data.valid_ds.items[idx].name)\n",
        "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "    data.valid_ds.x[idx].show(ax=ax1)\n",
        "    ax2.imshow(image2np(data.valid_ds.y[idx].data*255))\n",
        "    plt.show()\n",
        "    show_3ch(data.valid_ds.y[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tkhm3-EuvDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visually inspect data-augmented training images\n",
        "# TODO: show_batch doesn't display RGB mask correctly, setting alpha=0 to turn off for now\n",
        "data.show_batch(4,figsize=(10,10), alpha=0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puhcoHyUwNHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emLw4t8-w50t",
        "colab_type": "text"
      },
      "source": [
        "## Define custom losses and metrics to handle 3-channel targets\n",
        "\n",
        "Here we implement some new loss functions like Dice Loss and Focal Loss which have been shown to perform well in image segmentation tasks. Then we'll create a `MultiChComboLoss` class to combine multiple loss functions and  calculate them across the 3 channels with adjustable weighting.\n",
        "\n",
        "The approach of combining a Dice or Jaccard loss to consider image-wide context with individual pixel-focused Binary Cross Entropy or Focal loss with adjustable weighing of the 3 target mask channels has been shown to consistently outperform single loss functions. This is well-documented by Nick Weir's deep dive into the recent [SpaceNet 4 Off-Nadir Building Detection](https://spacenetchallenge.github.io/datasets/spacenet-OffNadir-summary.html) top results: \n",
        "\n",
        "https://medium.com/the-downlinq/a-deep-dive-into-the-spacenet-4-winning-algorithms-8d611a5dfe25\n",
        "\n",
        "Finally, we adapt our model evaluation metrics (accuracy and dice score) to calculate a mean score for all channels or by a specified individual channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ndnGpPVxBf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "\n",
        "def dice_loss(input, target):\n",
        "#     pdb.set_trace()\n",
        "    smooth = 1.\n",
        "    input = torch.sigmoid(input)\n",
        "    iflat = input.contiguous().view(-1).float()\n",
        "    tflat = target.contiguous().view(-1).float()\n",
        "    intersection = (iflat * tflat).sum()\n",
        "    return 1 - ((2. * intersection + smooth) / ((iflat + tflat).sum() +smooth))\n",
        "\n",
        "# adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets.float(), reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean': return F_loss.mean()\n",
        "        elif self.reduction == 'sum': return F_loss.sum()\n",
        "        else: return F_loss\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        loss = dice_loss(input, target)\n",
        "        if self.reduction == 'mean': return loss.mean()\n",
        "        elif self.reduction == 'sum': return loss.sum()\n",
        "        else: return loss\n",
        "\n",
        "class MultiChComboLoss(nn.Module):\n",
        "    def __init__(self, reduction='mean', loss_funcs=[FocalLoss(),DiceLoss()], loss_wts = [1,1], ch_wts=[1,1,1]):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        self.ch_wts = ch_wts\n",
        "        self.loss_wts = loss_wts\n",
        "        self.loss_funcs = loss_funcs \n",
        "        \n",
        "    def forward(self, output, target):\n",
        "#         pdb.set_trace()\n",
        "        for loss_func in self.loss_funcs: loss_func.reduction = self.reduction # need to change reduction on fwd pass for loss calc in learn.get_preds(with_loss=True)\n",
        "        loss = 0\n",
        "        channels = output.shape[1]\n",
        "        assert len(self.ch_wts) == channels\n",
        "        assert len(self.loss_wts) == len(self.loss_funcs)\n",
        "        for ch_wt,c in zip(self.ch_wts,range(channels)):\n",
        "            ch_loss=0\n",
        "            for loss_wt, loss_func in zip(self.loss_wts,self.loss_funcs): \n",
        "                ch_loss+=loss_wt*loss_func(output[:,c,None], target[:,c,None])\n",
        "            loss+=ch_wt*(ch_loss)\n",
        "        return loss/sum(self.ch_wts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXyws22VxkI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate metrics on one channel (i.e. ch 0 for building footprints only) or on all 3 channels\n",
        "\n",
        "def acc_thresh_multich(input:Tensor, target:Tensor, thresh:float=0.5, sigmoid:bool=True, one_ch:int=None)->Rank0Tensor:\n",
        "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
        "    \n",
        "#     pdb.set_trace()\n",
        "    if sigmoid: input = input.sigmoid()\n",
        "    n = input.shape[0]\n",
        "    \n",
        "    if one_ch is not None:\n",
        "        input = input[:,one_ch,None]\n",
        "        target = target[:,one_ch,None]\n",
        "    \n",
        "    input = input.view(n,-1)\n",
        "    target = target.view(n,-1)\n",
        "    return ((input>thresh)==target.byte()).float().mean()\n",
        "\n",
        "def dice_multich(input:Tensor, targs:Tensor, iou:bool=False, one_ch:int=None)->Rank0Tensor:\n",
        "    \"Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems.\"\n",
        "#     pdb.set_trace()\n",
        "    n = targs.shape[0]\n",
        "    input = input.sigmoid()\n",
        "    \n",
        "    if one_ch is not None:\n",
        "        input = input[:,one_ch,None]\n",
        "        targs = targs[:,one_ch,None]\n",
        "    \n",
        "    input = (input>0.5).view(n,-1).float()\n",
        "    targs = targs.view(n,-1).float()\n",
        "\n",
        "    intersect = (input * targs).sum().float()\n",
        "    union = (input+targs).sum().float()\n",
        "    if not iou: return (2. * intersect / union if union > 0 else union.new([1.]).squeeze())\n",
        "    else: return intersect / (union-intersect+1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDrR0C98xuRj",
        "colab_type": "text"
      },
      "source": [
        "## Set up model\n",
        "\n",
        "We'll set up fastai's Dynamic Unet model with an ImageNet-pretrained resnet34 encoder. This architecture, inspired by the original U-net, uses by default many advanced deep learning techniques such as:\n",
        "\n",
        "- One cycle learning schedule: https://sgugger.github.io/the-1cycle-policy.html\n",
        "- AdamW optimizer: https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
        "- Pixel shuffle upsampling with ICNR initiation from super resolution research: https://medium.com/@hirotoschwert/introduction-to-deep-super-resolution-c052d84ce8cf\n",
        "- Optionally set leaky ReLU, blur, self attention: https://docs.fast.ai/vision.models.unet.html#DynamicUnet\n",
        "\n",
        "We'll define our `MultiChComboLoss` function as a balanced combination of Focal Loss and Dice Loss and set our accuracy and dice metrics. \n",
        "\n",
        "Also note that our metrics displayed during training shows channel-0 (building footprint channel only) accuracy and dice metrics in the right-most 2 columns while the first two accuracy and dice metrics (left-hand columns) show the mean of the respective metric across all 3 channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqx14KUQxq5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up metrics to show mean metrics for all channels as well as the building-only metrics (channel 0)\n",
        "\n",
        "acc_ch0 = partial(acc_thresh_multich, one_ch=0)\n",
        "dice_ch0 = partial(dice_multich, one_ch=0)\n",
        "metrics = [acc_thresh_multich, dice_multich, acc_ch0, dice_ch0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgxSb2t_xxxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combo Focal + Dice loss with equal channel wts\n",
        "\n",
        "learn = unet_learner(data, models.resnet34, model_dir='../../models',\n",
        "                     metrics=metrics, \n",
        "                     loss_func=MultiChComboLoss(\n",
        "                        reduction='mean',\n",
        "                        loss_funcs=[FocalLoss(gamma=1, alpha=0.95),\n",
        "                                    DiceLoss(),\n",
        "                                   ], \n",
        "                        loss_wts=[1,1],\n",
        "                        ch_wts=[1,1,1])\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hWYX_elr4mkT",
        "colab": {}
      },
      "source": [
        "learn.metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9H6MlTOx4JK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.loss_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onPkAXvkx5aT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAeLTpwpyDfv",
        "colab_type": "text"
      },
      "source": [
        "## Train model, inspect results, unfreeze & train more, export for inference\n",
        "\n",
        "First, we'll fine-tune our Unet on the decoder part only (leaving the weights for the ImageNet-pretrained resnet34 encoder frozen) for some epochs. Then we'll unfreeze all the trainable weights/layers of our model and train for some more epochs.\n",
        "\n",
        "We'll track the `valid_loss`, `acc_...`, and `dice_..`. metrics per epoch as training progresses to make sure they continue to improve and we're not overfitting. And we set a `SaveModelCallback` which will track the channel-0 dice score, save a model checkpoint each time there's an improvement, and reload the highest performing model checkpoint file at the end of training.\n",
        "\n",
        "We'll also inspect our model's results by setting `learn.model.eval()`, generating some batches of predictions on the validation set, calculating and reshaping the image-wise loss values, and sorting by highest loss first to see the worst performing results (as measured by the loss which may differ in surprising ways from visually gauging results). \n",
        "\n",
        "**Pro-tip:** display and view your results every chance you get! You'll pick up on all kinds of interesting clues about your model's behavior and how to make it better.\n",
        "\n",
        "Finally, we'll export our trained Unet segmentation model for inference purposes as a `.pkl` file. Learn more about exporting fastai models for inference in this tutorial: https://docs.fast.ai/tutorial.inference.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMekweEBx7nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxWCHrWMyH5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.recorder.plot(0,2,suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsgQJ3uuZrsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = slice(3e-6, 3e-4)\n",
        "learn.fit_one_cycle(10, max_lr=lr, \n",
        "                    callbacks=[\n",
        "                        SaveModelCallback(learn,\n",
        "                                         monitor='dice_multich',\n",
        "                                         mode='max',\n",
        "                                         name='znz001trn-focaldice-stage1-best')\n",
        "                    ]\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyVPwPox2Zxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.model.eval()\n",
        "outputs,labels,losses = learn.get_preds(ds_type=DatasetType.Valid,n_batch=3,with_loss=True)\n",
        "losses.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K39jL-jX2r1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses_reshaped = torch.mean(losses.view(outputs.shape[0],-1), dim=1)\n",
        "sorted_idx = torch.argsort(losses_reshaped,descending=True)\n",
        "losses_reshaped.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPXDXaRS2l8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at predictions vs actual by channel sorted by highest image-wise loss first\n",
        "\n",
        "for i in sorted_idx[:10]:\n",
        "\n",
        "    print(f'{data.valid_ds.items[i].name}')\n",
        "    print(f'loss: {losses_reshaped[i].mean()}')\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "    \n",
        "    data.valid_ds.x[i].show(ax=ax1)\n",
        "    ax1.set_title('Prediction')\n",
        "    ax1.imshow(image2np(outputs[i].sigmoid()), alpha=0.4)\n",
        "    \n",
        "    ax2.set_title('Ground Truth')\n",
        "    data.valid_ds.x[i].show(ax=ax2)\n",
        "    ax2.imshow(image2np(labels[i])*255, alpha=0.4)\n",
        "    plt.show()\n",
        "    \n",
        "    print('Predicted:')\n",
        "    show_3ch(outputs[i].sigmoid())\n",
        "    print('Actual:')\n",
        "    show_3ch(labels[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOaTjX_x4BZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('znz001trn-focaldice-stage1-best')\n",
        "learn.model.train()\n",
        "learn.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VrmP3Aj4PKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhkJR61K8VDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot(suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUqZjKsA8yI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(20, max_lr=slice(3e-6,3e-4), \n",
        "                    callbacks=[\n",
        "                        SaveModelCallback(learn,\n",
        "                                           monitor='dice_multich',\n",
        "                                           mode='max',\n",
        "                                           name='znz001trn-focaldice-unfrozen-best')\n",
        "                    ]\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCKlE-qYAYeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.model.eval()\n",
        "outputs,labels,losses = learn.get_preds(ds_type=DatasetType.Valid,n_batch=6,with_loss=True)\n",
        "losses_reshaped = torch.mean(losses.view(outputs.shape[0],-1), dim=1)\n",
        "sorted_idx = torch.argsort(losses_reshaped,descending=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d-ZKoGqGAo9M",
        "colab": {}
      },
      "source": [
        "# look at predictions vs actual by channel sorted by highest image-wise loss first\n",
        "\n",
        "for i in sorted_idx[:10]:\n",
        "\n",
        "    print(f'{data.valid_ds.items[i].name}')\n",
        "    print(f'loss: {losses_reshaped[i].mean()}')\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "    \n",
        "    data.valid_ds.x[i].show(ax=ax1)\n",
        "    ax1.set_title('Prediction')\n",
        "    ax1.imshow(image2np(outputs[i].sigmoid()), alpha=0.4)\n",
        "    \n",
        "    ax2.set_title('Ground Truth')\n",
        "    data.valid_ds.x[i].show(ax=ax2)\n",
        "    ax2.imshow(image2np(labels[i])*255, alpha=0.4)\n",
        "    plt.show()\n",
        "    \n",
        "    print('Predicted:')\n",
        "    show_3ch(outputs[i].sigmoid())\n",
        "    print('Actual:')\n",
        "    show_3ch(labels[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRsgsHzEAqlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pickling with custom classes like MultiChComboLoss is a bit tricky \n",
        "learn.export('../../models/znz001trn-focaldice.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t0iiEBBV_fBj"
      },
      "source": [
        "## Save files to GDrive (or download to computer)\n",
        "\n",
        "Colab does not persistently store any files created and saved in its runtimes for than 8-12 hours (or less depending on inactivity or overall demand on the system). We'll transfer or download the files we create somewhere else. We can:\n",
        "\n",
        "1. **Mount our own account's Google Drive storage and transfer files** there via a `!cp` command: see below cell\n",
        "2. **Download  files to local computer:** go to Files tab on left > find and right-click selected file > click Download > file will be prepared by Colab and automatically downloaded when ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBBl1w8V_fBn",
        "colab": {}
      },
      "source": [
        "# to mount and transfer files to GDrive: uncomment and run this and the next cell, follow instructions to auhorize access to your GDrive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ngnUa0TN_fBq",
        "colab": {}
      },
      "source": [
        "# copy model export .pkl file to root of your GDrive\n",
        "# !cp models/znz001trn-focaldice.pkl /content/drive/My\\ Drive/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTquwBlYGR_U",
        "colab_type": "text"
      },
      "source": [
        "# Inference on new imagery\n",
        "\n",
        "With our segmentation model trained and exported for inference use, we will now re-load it as an inference-only model to test on new unseen imagery. We'll test the generalizability of our trained segmentation model on tiles from drone imagery captured over another part of Zanzibar and in other parts of the world as well as at varying `zoom_levels` (locations and zoom levels indicated):\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1200/1*DaS2dVfeaxZCg6cqOcHDrg.jpeg)\n",
        "\n",
        "We'll also compare our model inference time per tile on GPU versus CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcu5YXjFBfMe",
        "colab_type": "text"
      },
      "source": [
        "## Load exported model for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkQIj8icH4OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl https://course.fast.ai/setup/colab | bash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qISyNvJhIS3h",
        "colab": {}
      },
      "source": [
        "from fastai.vision import *\n",
        "from fastai.callbacks import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CK-4kz5vIS3m",
        "colab": {}
      },
      "source": [
        "from fastai.utils.collect_env import *\n",
        "show_install(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5HmNDT0JFq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: look into better way of loading export.pkl w/o needing to redefine these custom classes\n",
        "\n",
        "class SegLabelListCustom(SegmentationLabelList):\n",
        "    def open(self, fn): return open_mask(fn, div=True, convert_mode='RGB')\n",
        "    \n",
        "class SegItemListCustom(SegmentationItemList):\n",
        "    _label_cls = SegLabelListCustom\n",
        "\n",
        "def dice_loss(input, target):\n",
        "#     pdb.set_trace()\n",
        "    smooth = 1.\n",
        "    input = torch.sigmoid(input)\n",
        "    iflat = input.contiguous().view(-1).float()\n",
        "    tflat = target.contiguous().view(-1).float()\n",
        "    intersection = (iflat * tflat).sum()\n",
        "    return 1 - ((2. * intersection + smooth) / ((iflat + tflat).sum() +smooth))\n",
        "\n",
        "# adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets.float(), reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean': return F_loss.mean()\n",
        "        elif self.reduction == 'sum': return F_loss.sum()\n",
        "        else: return F_loss\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        loss = dice_loss(input, target)\n",
        "        if self.reduction == 'mean': return loss.mean()\n",
        "        elif self.reduction == 'sum': return loss.sum()\n",
        "        else: return loss\n",
        "\n",
        "class MultiChComboLoss(nn.Module):\n",
        "    def __init__(self, reduction='mean', loss_funcs=[FocalLoss(),DiceLoss()], loss_wts = [1,1], ch_wts=[1,1,1]):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        self.ch_wts = ch_wts\n",
        "        self.loss_wts = loss_wts\n",
        "        self.loss_funcs = loss_funcs \n",
        "        \n",
        "    def forward(self, output, target):\n",
        "#         pdb.set_trace()\n",
        "        for loss_func in self.loss_funcs: loss_func.reduction = self.reduction # need to change reduction on fwd pass for loss calc in learn.get_preds(with_loss=True)\n",
        "        loss = 0\n",
        "        channels = output.shape[1]\n",
        "        assert len(self.ch_wts) == channels\n",
        "        assert len(self.loss_wts) == len(self.loss_funcs)\n",
        "        for ch_wt,c in zip(self.ch_wts,range(channels)):\n",
        "            ch_loss=0\n",
        "            for loss_wt, loss_func in zip(self.loss_wts,self.loss_funcs): \n",
        "                ch_loss+=loss_wt*loss_func(output[:,c,None], target[:,c,None])\n",
        "            loss+=ch_wt*(ch_loss)\n",
        "        return loss/sum(self.ch_wts)\n",
        "\n",
        "def acc_thresh_multich(input:Tensor, target:Tensor, thresh:float=0.5, sigmoid:bool=True, one_ch:int=None)->Rank0Tensor:\n",
        "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
        "    \n",
        "#     pdb.set_trace()\n",
        "    if sigmoid: input = input.sigmoid()\n",
        "    n = input.shape[0]\n",
        "    \n",
        "    if one_ch is not None:\n",
        "        input = input[:,one_ch,None]\n",
        "        target = target[:,one_ch,None]\n",
        "    \n",
        "    input = input.view(n,-1)\n",
        "    target = target.view(n,-1)\n",
        "    return ((input>thresh)==target.byte()).float().mean()\n",
        "\n",
        "def dice_multich(input:Tensor, targs:Tensor, iou:bool=False, one_ch:int=None)->Rank0Tensor:\n",
        "    \"Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems.\"\n",
        "#     pdb.set_trace()\n",
        "    n = targs.shape[0]\n",
        "    input = input.sigmoid()\n",
        "    \n",
        "    if one_ch is not None:\n",
        "        input = input[:,one_ch,None]\n",
        "        targs = targs[:,one_ch,None]\n",
        "    \n",
        "    input = (input>0.5).view(n,-1).float()\n",
        "    targs = targs.view(n,-1).float()\n",
        "\n",
        "    intersect = (input * targs).sum().float()\n",
        "    union = (input+targs).sum().float()\n",
        "    if not iou: return (2. * intersect / union if union > 0 else union.new([1.]).squeeze())\n",
        "    else: return intersect / (union-intersect+1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmaS5qy_NnFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O models/znz001trn-focaldice.pkl https://www.dropbox.com/s/by3nc1xri8y7t4p/znz001trn-focaldice.pkl?dl=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj32sr-m_zPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if you have your own model .pkl file to load, either:\n",
        "\n",
        "# upload from computer: Files tab > Upload on left\n",
        "# or mount GDrive and transfer file to Colab storage: uncomment below, change filepaths to the .pkl file on your GDrive if needed, and run:\n",
        "\n",
        "# !cp /content/drive/My\\ Drive/znz001trn-focaldice.pkl models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n0jgRUiBFI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inference_learner = load_learner(path='models/', file='znz001trn-focaldice.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjGgCQUAJ5qN",
        "colab_type": "text"
      },
      "source": [
        "## Inference on new unseen tiles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFPOefyYCX2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skimage \n",
        "import time\n",
        "\n",
        "def get_pred(learner, tile):\n",
        "#     pdb.set_trace()\n",
        "    t_img = Image(pil2tensor(tile[:,:,:3],np.float32).div_(255))\n",
        "    outputs = learner.predict(t_img)\n",
        "    im = image2np(outputs[2].sigmoid())\n",
        "    im = (im*255).astype('uint8')\n",
        "    return im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w8TbLjkCePS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try a different tile by changing or adding your own urls to list\n",
        "\n",
        "urls = [\n",
        "  'https://tiles.openaerialmap.org/5b1009f22b6a08001185f24a/0/5b1009f22b6a08001185f24b/19/319454/270706.png',\n",
        "  'https://tiles.openaerialmap.org/5b1e6fd42b6a08001185f7bf/0/5b1e6fd42b6a08001185f7c0/20/569034/537093.png',\n",
        "  'https://tiles.openaerialmap.org/5beaaba463f9420005ef8db0/0/5beaaba463f9420005ef8db1/19/313479/283111.png',\n",
        "  'https://tiles.openaerialmap.org/5d050c3673de290005853a91/0/5d050c3673de290005853a92/18/203079/117283.png',\n",
        "  'https://tiles.openaerialmap.org/5c88ff77225fc20007ab4e26/0/5c88ff77225fc20007ab4e27/21/1035771/1013136.png',\n",
        "  'https://tiles.openaerialmap.org/5d30bac2e757aa0005951652/0/5d30bac2e757aa0005951653/19/136700/197574.png'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaJbaUvutIRb",
        "colab_type": "text"
      },
      "source": [
        "### On GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBSiA2hmEbxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for url in urls:\n",
        "  t1 = time.time()\n",
        "  test_tile = skimage.io.imread(url)\n",
        "  result = get_pred(inference_learner, test_tile)\n",
        "  t2 = time.time()\n",
        "  \n",
        "  print(url)\n",
        "  print(f'GPU inference took {t2-t1:.2f} secs')\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "  ax1.imshow(test_tile)\n",
        "  ax2.imshow(result)\n",
        "  ax1.axis('off')\n",
        "  ax2.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QVHwPZDtNOq",
        "colab_type": "text"
      },
      "source": [
        "### On CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHgJSFjWEvlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for url in urls:\n",
        "  t1 = time.time()\n",
        "  test_tile = skimage.io.imread(url)\n",
        "  print(url)\n",
        "  result = get_pred(inference_learner, test_tile)\n",
        "\n",
        "  t2 = time.time()\n",
        "  \n",
        "  print(f'CPU inference took {t2-t1:.2f} secs')\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "  ax1.imshow(test_tile)\n",
        "  ax2.imshow(result)\n",
        "  ax1.axis('off')\n",
        "  ax2.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOOZHgrnHBpz",
        "colab_type": "text"
      },
      "source": [
        "# Post-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vfnF54gbFlH",
        "colab_type": "text"
      },
      "source": [
        "## Predict on a tile, threshold, polygonize, and georegister\n",
        "\n",
        "For good evaluation of model performance against ground truth, we'll use another set of labeled data that the model was not trained on. We'll  get this from the larger Zanzibar dataset. Preview the imagery and ground truth labels for `znz029` in the STAC browser [here](https://geoml-samples.netlify.com/item/9Eiufow7wPXLqQEP1Di2J5X8kXkBLgMsCBoN37VrtRPB/2sEaEKnnyjG2mx7CnN1ESAdjYAEQjoNRxT2vgQRC9oB?si=0&t=preview#14/-5.865178/39.348986):\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1200/0*fGsRIu-2ExIWXzc0)\n",
        "\n",
        "For demonstration, we'll use this particular tile at `z=19, x=319454, y=270706` from `znz029`:\n",
        "\n",
        "![alt text](https://tiles.openaerialmap.org/5b1009f22b6a08001185f24a/0/5b1009f22b6a08001185f24b/19/319454/270706.png)\n",
        "\n",
        "Using solaris and geopandas, we'll convert our model's prediction as a 3-channel pixel raster output into a GeoJSON file by:\n",
        "\n",
        "1. thresholding and combining the 3-channels of pixel values in our raw prediction output into a 1 channel binary pixel mask\n",
        "2. polygonizing this binary pixel mask into shape vectors representing the predicted footprint of every building\n",
        "3. georegistering the x, y display coordinates of these vectorized building shapes into longitude, latitude coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZcaSvj6daHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if not already loaded in runtime: \n",
        "# install fastai and load inference learner from \"Inference on new imagery section\" \n",
        "# and uncomment below and re-install geo packages\n",
        "\n",
        "# !add-apt-repository ppa:ubuntugis/ppa\n",
        "# !apt-get update\n",
        "# !apt-get install python-numpy gdal-bin libgdal-dev\n",
        "# !apt install python3-rtree\n",
        "\n",
        "# !pip install rasterio\n",
        "# !pip install geopandas\n",
        "# !pip install descartes\n",
        "# !pip install solaris\n",
        "# !pip install rio-tiler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQqT1nv0dYiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import solaris as sol \n",
        "from affine import Affine\n",
        "from rasterio.transform import from_bounds\n",
        "from shapely.geometry import Polygon\n",
        "import math\n",
        "import geopandas as gpd\n",
        "import skimage\n",
        "  \n",
        "def deg2num(lat_deg, lon_deg, zoom):\n",
        "    lat_rad = math.radians(lat_deg)\n",
        "    n = 2.0 ** zoom\n",
        "    xtile = int((lon_deg + 180.0) / 360.0 * n)\n",
        "    ytile = int((1.0 - math.log(math.tan(lat_rad) + (1 / math.cos(lat_rad))) / math.pi) / 2.0 * n)\n",
        "    return (xtile, ytile)\n",
        "\n",
        "def num2deg(xtile, ytile, zoom):\n",
        "    n = 2.0 ** zoom\n",
        "    lon_deg = xtile / n * 360.0 - 180.0\n",
        "    lat_rad = math.atan(math.sinh(math.pi * (1 - 2 * ytile / n)))\n",
        "    lat_deg = math.degrees(lat_rad)\n",
        "    return (lat_deg, lon_deg)\n",
        "  \n",
        "def tile_to_poly(z,x,y, size):\n",
        "    top, left = num2deg(x, y, z)\n",
        "    bottom, right = num2deg(x+1, y+1, z)\n",
        "    tfm = from_bounds(left, bottom, right, top, size, size)\n",
        "\n",
        "    return Polygon.from_bounds(left,top,right,bottom), tfm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB4l7hcVbELe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z,x,y = 19,319454,270706\n",
        "url= 'https://tiles.openaerialmap.org/5b1009f22b6a08001185f24a/0/5b1009f22b6a08001185f24b/19/319454/270706.png'\n",
        "\n",
        "test_tile = skimage.io.imread(url)\n",
        "result = get_pred(inference_learner, test_tile)\n",
        "  \n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "ax1.imshow(test_tile)\n",
        "ax2.imshow(result)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlyLmhGimkus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# threshold and polygonize with solaris: https://solaris.readthedocs.io/en/latest/tutorials/notebooks/api_mask_to_vector.html\n",
        "\n",
        "mask2poly = sol.vector.mask.mask_to_poly_geojson(result, \n",
        "                                                 channel_scaling=[1,0,-1], \n",
        "                                                 bg_threshold=245, \n",
        "                                                 simplify=True,\n",
        "                                                 tolerance=2\n",
        "                                                 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIk_zl2NdYfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask2poly.plot(figsize=(10,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1iNz7bjh62e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask2poly.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haymUIyBjjyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the bounds of the tile and its affine tfm matrix for georegistering purposes\n",
        "tile_poly, tile_tfm = tile_to_poly(z,x,y,256)\n",
        "tile_tfm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgKInbWcinLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert polys from pixel coords to geo coords: https://solaris.readthedocs.io/en/latest/api/vector.html?highlight=georegister_px_df#solaris.vector.polygon.georegister_px_df\n",
        "result_polys = sol.vector.polygon.georegister_px_df(mask2poly, \n",
        "                                                   affine_obj=tile_tfm, \n",
        "                                                   crs='epsg:4326')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXDN0CJnlTc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show tile image to raw prediction to georegistered polygons\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
        "\n",
        "ax1.imshow(test_tile)\n",
        "ax2.imshow(result)\n",
        "result_polys.plot(ax=ax3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzyF9NTylcjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_polys.to_file('result_polys.geojson', driver='GeoJSON')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M_QpAlBnNXT",
        "colab_type": "text"
      },
      "source": [
        "### Check that the saved result_poly.geojson is correctly georegistered on geojson.io\n",
        "\n",
        "http://geojson.io/#id=gist:daveluo/3dfe4695e31b2b3a4c7c6e13ada5d1e6&map=19/-5.86910/39.35198\n",
        "\n",
        "TMS layer link: https://tiles.openaerialmap.org/5ae242fd0b093000130afd38/0/5ae242fd0b093000130afd39/{z}/{x}/{y}.png\n",
        "\n",
        "![alt text](https://www.dropbox.com/s/kxideja8cx1ao14/check_predicted_polys.png?dl=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sndI40Vh8sIy",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate prediction against ground truth\n",
        "\n",
        "Finally with georegistered building predictions as a GeoJSON file, we can evaluate it against the ground truth GeoJSON file for the same tile.\n",
        "\n",
        "We'll clip the ground truth labels to the bounds of this particular tile and use solaris's Evaluator to calculate the precision, recall, and F1 score. We will also visualize our predicted buildings (in red) against the ground truth buildings (in blue) in this particular tile.\n",
        "\n",
        "For more information about  these common evaluation metrics for models applied to overhead imagery, see the following articles and more by the SpaceNet team:\n",
        "\n",
        "https://medium.com/the-downlinq/the-spacenet-metric-612183cc2ddb\n",
        "\n",
        "https://medium.com/the-downlinq/the-good-and-the-bad-in-the-spacenet-off-nadir-building-footprint-extraction-challenge-4c3a96ee9c72"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44li50wXlQ6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the ground truth labels for all znz029\n",
        "labels_url = 'https://www.dropbox.com/sh/ct3s1x2a846x3yl/AADHytc8fSCf3gna0wNAW3lZa/grid_029.geojson?dl=1'\n",
        "\n",
        "gt_gdf = gpd.read_file(labels_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36UNPjfJvyJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tile_poly.bounds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f46pPOKxpe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the tile (in red) against the entire labeled znz029 area (in blue)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "gt_gdf.plot(ax=ax)\n",
        "gpd.GeoDataFrame(geometry=[tile_poly], crs='epsg:4326').plot(alpha=0.5, color='red', ax=ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiGM59ouxihG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clip gt_gdf to the tile bounds\n",
        "clipped_gt_polys = gpd.overlay(gt_gdf, gpd.GeoDataFrame(geometry=[tile_poly], crs='epsg:4326'), how='intersection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVf23O_syH7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clipped_gt_polys.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ZIdUBCwScp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_polys.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFWPF40trtbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clipped_gt_polys.to_file('clipped_gt_polys.geojson', driver='GeoJSON')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCd9pyKir0kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# solaris tutorial on evaluation: https://solaris.readthedocs.io/en/latest/tutorials/notebooks/api_evaluation_tutorial.html \n",
        "evaluator = sol.eval.base.Evaluator('clipped_gt_polys.geojson')\n",
        "evaluator.load_proposal('result_polys.geojson', proposalCSV=False, conf_field_list=[])\n",
        "evaluator.eval_iou(calculate_class_scores=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jM68x8quFWD3",
        "colab": {}
      },
      "source": [
        "# visualize predicted vs ground truth\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "\n",
        "ax1.imshow(test_tile)\n",
        "clipped_gt_polys.plot(ax=ax2, color='blue', alpha=0.5) #gt\n",
        "result_polys.plot(ax=ax2, color='red', alpha=0.5) #pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar3C2MS8zGHF",
        "colab_type": "text"
      },
      "source": [
        "# Ideas to Try for Performance Gains\n",
        "\n",
        "Congratulations, you did it! \n",
        "\n",
        "You've completed the tutorial and now know how to do everything from producing training data to creating a deep learning model for segmentation to postprocessing and evaluating your model's performance.\n",
        "\n",
        "To flex your newfound knowledge and make your model perform potentially **much better**, try implementing some or all these ideas:\n",
        "\n",
        "- Create and use more training data: there are 13 grids' worth of training data for Zanzibar released as part of the [Open AI Tanzania Building Footprint Segmentation Challenge dataset](https://docs.google.com/spreadsheets/d/1kHZo2KA0-VtCCcC5tL4N0SpyoxnvH7mLbybZIHZGTfE/edit#gid=0).\n",
        "\n",
        "- Change the zoom_level of your training/validation tiles. Better yet, try using tiles across multiple zooms (i.e. z21, z20, z19, z18). Note that with multiple zoom levels over the same imagery, you should be extra careful of overlapping tiles across those different zoom levels. ← test your understanding of slippy map tiles by checking that you understand what I mean here but feel free to message me for the answer!\n",
        "\n",
        "- Change the Unet's encoder to a bigger or different architecture (i.e. resnet50, resnet101, densenet). \n",
        "\n",
        "- Change the combinations, weighting, and hyperparameters of the loss functions. Or implement completely new loss functions like [Lovasz Loss](https://github.com/bermanmaxim/LovaszSoftmax).\n",
        "\n",
        "- Try different data augmentation combinations and techniques.\n",
        "\n",
        "- Train for more epochs and with different learning rate schedules. Try [mixed-precision](https://docs.fast.ai/callbacks.fp16.html) for faster model training. \n",
        "\n",
        "- Your idea here.\n",
        "\n",
        "I look forward to seeing what you discover!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgU3_2_3Fe1B",
        "colab_type": "text"
      },
      "source": [
        "# Coming Up\n",
        "\n",
        "If you liked this tutorial, look forward to next ones which will potentially cover topics like:\n",
        "- classifying building completeness (foundation, incomplete, complete)\n",
        "- inference on multiple tiles and much larger images\n",
        "- working with messy, sparse, imperfect training data\n",
        "- model deployment and inference at scale\n",
        "- examining data/model biases, considerations of fairness, accountability, transparency, and ethics\n",
        "\n",
        "Curious about more geospatial deep learning topics? Did I miss something? Share your questions and thoughts in the [Medium post](https://medium.com/@anthropoco/how-to-segment-buildings-on-drone-imagery-with-fast-ai-cloud-native-geodata-tools-ae249612c321?source=friends_link&sk=57b82002ac47724ecf9a2aaa98de994b) so I can add them into this and next tutorials. \n",
        "\n",
        "Good luck and happy deep learning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m7i9fXQkA0z",
        "colab_type": "text"
      },
      "source": [
        "# Acknowledgments and Special Thanks to\n",
        "\n",
        "- [World Bank GFDRR](https://www.gfdrr.org/en)'s Open Data for Resilience\n",
        "Initiative ([OpenDRI](https://opendri.org/)) for consultation projects which have inspired & informed.\n",
        "- [Zanzibar Mapping Initiative](http://www.zanzibarmapping.com/), [OpenAerialMap](https://openaerialmap.org/), State University of Zanzibar ([SUZA](https://www.suza.ac.tz/)), Govt of Zanzibar's Commission for Lands, & [WeRobotics](https://werobotics.org/) for the [2018 Open AI Tanzania Building Footprint Segmentation Challenge](https://competitions.codalab.org/competitions/20100).\n",
        "- [Fast.ai team](https://www.fast.ai/about/), [contributors](https://github.com/fastai/fastai/graphs/contributors), & [community](https://forums.fast.ai/) for both \"making neural nets uncool again\" and pushing its cutting edge (very cool).\n",
        "- [SpaceNet](https://spacenet.ai/) & [Cosmiq Works](http://www.cosmiqworks.org/) for the open challenges, datasets, knowledge-sharing, [Solaris geoML toolkit](https://github.com/CosmiQ/solaris), & more that advance geospatial machine learning.\n",
        "- Contributors to [COG](https://www.cogeo.org/), [STAC](https://stacspec.org/), and more initiatives advancing the [cloud native geospatial](https://medium.com/planet-stories/tagged/cloud-native-geospatial) ecosystem.\n",
        "- [Free & open source](https://en.wikipedia.org/wiki/Free_and_open-source_software) creators & collaborators everywhere for the invaluable public goods you provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpDzngDcNFg-",
        "colab_type": "text"
      },
      "source": [
        "# Notebook Changelog\n",
        "\n",
        "## v1 (2019-07-25):\n",
        "-------\n",
        "New: 1st public release\n",
        "\n",
        "Changed: n/a\n",
        "\n",
        "Fixed: n/a"
      ]
    }
  ]
}